{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE6qcw_j8Pi2"
   },
   "source": [
    "In this homework assignment, you are requested to implement a full backprop algorithm using only *numpy*.\n",
    "\n",
    "- We assume sigmoid activation across all layers.\n",
    "- We assume a single value in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "UV4RvXYL8k85"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRml6glFIPCa"
   },
   "source": [
    "The following class represents a simple feed forward network with multiple layers. The network class provides methods for running forward and backward for a single instance, throught the network. You should implement the methods (indicated with TODO), that performs forward and backward for an entire batch. Note, the idea is to use matrix multiplications, and not running standard loops over the instances in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "kLdNoCt58qg5"
   },
   "outputs": [],
   "source": [
    "class MyNN:\n",
    "  def __init__(self, learning_rate, layer_sizes):\n",
    "    '''\n",
    "    learning_rate - the learning to use in backward\n",
    "    layer_sizes - a list of numbers, each number repreents the nuber of neurons\n",
    "                  to have in every layer. Therfore, the length of the list\n",
    "                  represents the number layers this network has.\n",
    "    '''\n",
    "    self.learning_rate = learning_rate\n",
    "    self.layer_sizes = layer_sizes\n",
    "    self.model_params = {}\n",
    "    self.memory = {}\n",
    "    self.grads = {}\n",
    "\n",
    "    # Initializing weights\n",
    "    for layer_index in range(len(layer_sizes) - 1):\n",
    "      W_input = layer_sizes[layer_index + 1]\n",
    "      W_output = layer_sizes[layer_index]\n",
    "      self.model_params['W_' + str(layer_index + 1)] = np.random.randn(W_input, W_output) * 0.1\n",
    "      self.model_params['b_' + str(layer_index + 1)] = np.random.randn(W_input) * 0.1\n",
    "\n",
    "\n",
    "  def forward_single_instance(self, x):\n",
    "    a_i_1 = x\n",
    "    self.memory['a_0'] = x\n",
    "    for layer_index in range(len(self.layer_sizes) - 1):\n",
    "      W_i = self.model_params['W_' + str(layer_index + 1)]\n",
    "      b_i = self.model_params['b_' + str(layer_index + 1)]\n",
    "      z_i = np.dot(W_i, a_i_1) + b_i\n",
    "      a_i = 1/(1+np.exp(-z_i))\n",
    "      self.memory['a_' + str(layer_index + 1)] = a_i\n",
    "      a_i_1 = a_i\n",
    "    return a_i_1\n",
    "\n",
    "\n",
    "  def log_loss(self, y_hat, y):\n",
    "    '''\n",
    "    Logistic loss, assuming a single value in y_hat and y.\n",
    "    '''\n",
    "    m = y_hat[0]\n",
    "    cost = -y[0]*np.log(y_hat[0]) - (1 - y[0])*np.log(1 - y_hat[0])\n",
    "    return cost\n",
    "\n",
    "\n",
    "  def backward_single_instance(self, y):\n",
    "    a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]\n",
    "    dz = a_output - y\n",
    "\n",
    "    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
    "      print(layer_index)\n",
    "      a_l_1 = self.memory['a_' + str(layer_index - 1)]\n",
    "      dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1))\n",
    "      self.grads['dW_' + str(layer_index)] = dW\n",
    "      W_l = self.model_params['W_' + str(layer_index)]\n",
    "      dz = (a_l_1 * (1 - a_l_1)).reshape(-1, 1) * np.dot(W_l.T, dz.reshape(-1, 1))\n",
    "      # TODO: calculate and memorize db as well.\n",
    "\n",
    "  # TODO: update weights with grads\n",
    "  #def update(self):\n",
    "\n",
    "  # TODO: implement forward for a batch X.shape = (network_input_size, number_of_instance)\n",
    "  #def forward_batch(self, X)\n",
    "\n",
    "  # TODO: implement backward for a batch y.shape = (1, number_of_instance)\n",
    "  #def backward_batch(self, y)\n",
    "\n",
    "  # TODO: implement log_loss_batch, for a batch of instances\n",
    "  # def log_loss(self, y_hat, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Qib6W4QXO644"
   },
   "outputs": [],
   "source": [
    "nn = MyNN(0.01, [3, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4nQR8QllPf_5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_1': array([[ 0.04967142, -0.01382643,  0.06476885],\n",
       "        [ 0.15230299, -0.02341534, -0.0234137 ]]),\n",
       " 'b_1': array([0.15792128, 0.07674347]),\n",
       " 'W_2': array([[-0.04694744,  0.054256  ]]),\n",
       " 'b_2': array([-0.04634177])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VXiyn-yrPC6-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48946]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3)\n",
    "y = np.random.randn(1)\n",
    "\n",
    "y_hat = nn.forward_single_instance(x)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "k5M50i3plclj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "nn.backward_single_instance(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "CWnZB1YmYnIt"
   },
   "outputs": [],
   "source": [
    "def train(X, y, epochs, batch_size):\n",
    "  '''\n",
    "  Train procedure, please note the TODOs inside\n",
    "  '''\n",
    "  for e in range(1, epochs + 1):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Shuffle the numpy array but keep the same order of shuffeling between the 1-axis arrays\n",
    "    permutation = np.random.permutation(X.shape[1])\n",
    "    X = np.array([sub_x[permutation] for sub_x in X])\n",
    "    y = y[0][permutation].reshape(1, X.shape[1])\n",
    "\n",
    "    # Split the numpy arrays into batches\n",
    "    chunks = np.arange(batch_size,X.shape[1],batch_size)\n",
    "    X_chunks = np.split(X, chunks, 1)\n",
    "    y_chunks = np.split(y, chunks, 1)\n",
    "    batches = [(X_b, y_b) for X_b, y_b in zip(X_chunks, y_chunks)]\n",
    "      \n",
    "    for X_b, y_b in batches:\n",
    "      y_hat = nn.forward_batch(X_b)\n",
    "      epoch_loss += nn.log_loss_batch(y_hat, y_b)\n",
    "      nn.backward_batch(y_b)\n",
    "      nn.update()\n",
    "    print(f'Epoch {e}, loss={epoch_loss/len(batches)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "cE1ydWlatkty"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyNN' object has no attribute 'forward_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#print(f\"X is {X}\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(X.shape)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#print(f\"y is {y}\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print(y.shape)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, y, epochs, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m batches \u001b[38;5;241m=\u001b[39m [(X_b, y_b) \u001b[38;5;28;01mfor\u001b[39;00m X_b, y_b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_chunks, y_chunks)]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_b, y_b \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m---> 20\u001b[0m   y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_batch\u001b[49m(X_b)\n\u001b[1;32m     21\u001b[0m   epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mlog_loss_batch(y_hat, y_b)\n\u001b[1;32m     22\u001b[0m   nn\u001b[38;5;241m.\u001b[39mbackward_batch(y_b)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyNN' object has no attribute 'forward_batch'"
     ]
    }
   ],
   "source": [
    "# TODO: Make sure the following network trains properly\n",
    "\n",
    "nn = MyNN(0.001, [6, 4, 3, 1])\n",
    "\n",
    "X = np.random.randn(6, 100)\n",
    "y = np.random.randn(1, 100)\n",
    "batch_size = 8\n",
    "epochs = 2\n",
    "\n",
    "#print(f\"X is {X}\")\n",
    "#print(X.shape)\n",
    "#print(f\"y is {y}\")\n",
    "#print(y.shape)\n",
    "\n",
    "train(X, y, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dY4scUksulC"
   },
   "source": [
    "#TODO: train on an external dataset\n",
    "\n",
    "Train on the *hour.csv* file with a split of 75% training 10% validation and 15% for test.\n",
    "Use the following features from the data:\n",
    "\n",
    "* temp\n",
    "* atemp\n",
    "* hum\n",
    "* windspeed\n",
    "* weekday\n",
    "\n",
    "The response variable is, *success*\n",
    "\n",
    "The architecture of the network should be: [5, 40, 30, 10, 7, 5, 3, 1].\n",
    "\n",
    "Use batch_size=8, and train it for 100 epochs on the train set (based on the split as requested above).\n",
    "\n",
    "Then, plot train and validation loss per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKAxdO2I1IGT"
   },
   "source": [
    "##  your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHGoWEJk1K7r"
   },
   "source": [
    "###  Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "e4Ra8vc_1NCE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01/01/2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>01/01/2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>01/01/2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>01/01/2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>01/01/2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17374</th>\n",
       "      <td>17375</td>\n",
       "      <td>31/12/2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>11</td>\n",
       "      <td>108</td>\n",
       "      <td>119</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17375</th>\n",
       "      <td>17376</td>\n",
       "      <td>31/12/2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17376</th>\n",
       "      <td>17377</td>\n",
       "      <td>31/12/2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17377</th>\n",
       "      <td>17378</td>\n",
       "      <td>31/12/2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17378</th>\n",
       "      <td>17379</td>\n",
       "      <td>31/12/2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17379 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       instant      dteday  season  yr  mnth  hr  holiday  weekday  \\\n",
       "0            1  01/01/2011       1   0     1   0        0        6   \n",
       "1            2  01/01/2011       1   0     1   1        0        6   \n",
       "2            3  01/01/2011       1   0     1   2        0        6   \n",
       "3            4  01/01/2011       1   0     1   3        0        6   \n",
       "4            5  01/01/2011       1   0     1   4        0        6   \n",
       "...        ...         ...     ...  ..   ...  ..      ...      ...   \n",
       "17374    17375  31/12/2012       1   1    12  19        0        1   \n",
       "17375    17376  31/12/2012       1   1    12  20        0        1   \n",
       "17376    17377  31/12/2012       1   1    12  21        0        1   \n",
       "17377    17378  31/12/2012       1   1    12  22        0        1   \n",
       "17378    17379  31/12/2012       1   1    12  23        0        1   \n",
       "\n",
       "       workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
       "0               0           1  0.24  0.2879  0.81     0.0000       3   \n",
       "1               0           1  0.22  0.2727  0.80     0.0000       8   \n",
       "2               0           1  0.22  0.2727  0.80     0.0000       5   \n",
       "3               0           1  0.24  0.2879  0.75     0.0000       3   \n",
       "4               0           1  0.24  0.2879  0.75     0.0000       0   \n",
       "...           ...         ...   ...     ...   ...        ...     ...   \n",
       "17374           1           2  0.26  0.2576  0.60     0.1642      11   \n",
       "17375           1           2  0.26  0.2576  0.60     0.1642       8   \n",
       "17376           1           1  0.26  0.2576  0.60     0.1642       7   \n",
       "17377           1           1  0.26  0.2727  0.56     0.1343      13   \n",
       "17378           1           1  0.26  0.2727  0.65     0.1343      12   \n",
       "\n",
       "       registered  cnt  success  \n",
       "0              13   16    False  \n",
       "1              32   40    False  \n",
       "2              27   32    False  \n",
       "3              10   13    False  \n",
       "4               1    1    False  \n",
       "...           ...  ...      ...  \n",
       "17374         108  119    False  \n",
       "17375          81   89    False  \n",
       "17376          83   90    False  \n",
       "17377          48   61    False  \n",
       "17378          37   49    False  \n",
       "\n",
       "[17379 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24   0.22   0.22   ... 0.26   0.26   0.26  ]\n",
      " [0.2879 0.2727 0.2727 ... 0.2576 0.2727 0.2727]\n",
      " [0.81   0.8    0.8    ... 0.6    0.56   0.65  ]\n",
      " [0.     0.     0.     ... 0.1642 0.1343 0.1343]\n",
      " [6.     6.     6.     ... 1.     1.     1.    ]]\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "(5, 17379)\n",
      "(1, 17379)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"hour.csv\")\n",
    "\n",
    "display(df)\n",
    "\n",
    "features = df[[\"temp\", \"atemp\", \"hum\", \"windspeed\", \"weekday\"]].to_numpy()\n",
    "features = np.transpose(features)\n",
    "print(features)\n",
    "\n",
    "classification = np.array(df['success']).astype(int)\n",
    "classification = np.expand_dims(classification, axis=0)\n",
    "print(classification)\n",
    "\n",
    "print(features.shape)\n",
    "print(classification.shape)\n",
    "\n",
    "# TODO: Preprocess the bike sharing dataset ('hour.csv')\n",
    "# - Load the dataset from the provided hour.csv file\n",
    "# - Select the required features (temp, atemp, hum, windspeed, weekday)\n",
    "# - Extract the target variable (success)\n",
    "# - Normalize/standardize features if necessary\n",
    "# - Split the data into training (75%), validation (10%), and test (15%) sets\n",
    "# - Create DataLoader objects with batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glp6Vz3B1TZo"
   },
   "source": [
    "### Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXlKl21D1UGn"
   },
   "outputs": [],
   "source": [
    "# TODO: Train the neural network\n",
    "# - Implement the network with architecture [5, 40, 30, 10, 7, 5, 3, 1]\n",
    "# - Train for exactly 100 epochs on the training set\n",
    "# - Use batch_size=8 as specified\n",
    "# - Calculate and store train and validation loss for each epoch\n",
    "# - Track training progres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwBcg1yy1b-j"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqE6Jhxj1etd"
   },
   "outputs": [],
   "source": [
    "# TODO: Create visualizations of the learning process\n",
    "# - Plot the training loss per epoch\n",
    "# - Create additional relevant plots (validation loss, learning curves, etc.)\n",
    "# - Make sure all plots have proper labels, titles, and legends\n",
    "# - Add brief analysis of what the plots reveal about your model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQIAgWhe1i_2"
   },
   "source": [
    "### Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfquqstM1iIO"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate model performance on the test set\n",
    "# - Calculate and report the loss on the test set\n",
    "# - Calculate and report the accuracy on the test set\n",
    "# - Compare test performance with training/validation performance\n",
    "# - Analyze model strengths and weaknesses\n",
    "# - Discuss any overfitting/underfitting issues observed"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stav-kernel",
   "language": "python",
   "name": "stav-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
